---
date: 2020-12-21T02:54
---

# t.test explainer

[[[stattests]]]
[[R]]
[[Statistics]]

[Null testing explainer](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading18.pdf)

**Compare outcomes for two groups.**

In `R` it helps to recall school algebra: $$f(x) = y$$, where the three objects (in `R`, *everything* is an object), are

$x$, which is what's at hand, in this case the contingency tables
$y$, which is what's desired, in this case the result of some statistical test
$f$, is the function that provides the return $y$ value from $x$

Because objects, including functions, can contain other objects, working a problem in `R` is mainly an exercise in understanding what can be done with $x$, what $y$ has to look like and what function $f$ is available or can be composed to work the transformation. We say that $f$ is composable&mdash;functions called *first-class* objects, because they can be the arguments to other functions, just like school $f(g(x))$.

Statistics benefits from a similar approach, which you started by framing the questions as

> Is there a group difference in pos pre scores, etc.

is a good place to start because it immediately raises another question:

> Of course there is a difference&mdash;the group's aren't identical, so what kind of difference is of interest? Mean, median, range, `quantiles`?

Fortunately, the analysis is the same for all four questions. Start by reframing the question:

>  Are the differences between groups merely random?

This leads naturally to the framing of dueling hypothesis,  the null and alternative hypothesis 

> $H_0$ the *null hypothesis*: there is *no* difference
> $H_1$ the *alternative hypothesis*: there *is* a difference

How to tell?

We need a yardstick for inter-group differences, one that provides both a *test statistic* and a criterion by which we can decide if the value of the test statistic at a given level of confidence, called $\alpha$ requires us to accept the null and reject the alternative or vice-versa. The probability $P$ equals $1-\alpha$ and is unfortunately named "significance." A principled approach requires selecting $\alpha$ in advance, and a conventional choice is $\alpha = 0.05$ often shorthanded as a "95% confidence interval." (If you think of it though, you wouldn't want to get into a drinking game where the loser has to hold their choice of one of four five-shot revolvers with a single bullet among them, point it to their head and pull the trigger. That's why I call $\alpha = 0.05$ passing the laugh test.)

Looking at this in terms of the data, we have a collection of scores from two groups to look at with respect to `pos` or `neg` and `pre` or `post`. Let's look at just one: group1 vs group2 pos pre scores.

scores can take possible values of `c(0,1,2,3,4,5,5.5,6,7,9)` and `events` have possible values of `c(v1pre, v1post, v2pre, v2post)`.  The cross tabulations *should* show

```
 slots   <- c(0,1,2,3,4,5,5.5,6,7,9)
 v1pre1  <- c(3,1,0,0,0,0,0.0,0,1,0) # group1
 v1pre2  <- c(2,1,0,0,0,1,0.0,1,0,0) # group2
```
(We'll need to circle back later to fix $x$, because the contingency tables only show the scores for each group separately because the group scores don't completely overlap, but let's focus on finding the right test, first, using the two v1pre vectors.)

As before, let's see what kinds of differences exist, because if they're identical $\dots$

``` r
v1pre1  <- c(3,1,0,0,0,0,0.0,0,1,0) # group1
v1pre2  <- c(2,1,0,0,0,1,0.0,1,0,0) # group2
v1pre1 - v1pre2
#>  [1]  1  0  0  0  0 -1  0 -1  1  0
```

So, good, they are different somehow. But different how?

Here we need one of the foundational tools of statistics&mdash;*aggregation*, a reduction of  many observation into a single number. It could be anything, evens and odds say, but it's hard to argue with the `mean`.

`R` has a built-in function for determining the probability that the difference in means of two vectors is equal to zero. I'm going to fudge the data a little because 

``` r
# actual data from the example (padded to equal length)
v1pre1  <- c(3,1,0,0,0,0,0.0,0,1,0) # group1
v1pre2  <- c(2,1,0,0,0,1,0.0,1,0,0) # group2
mean(v1pre1) == mean(v1pre2)
#> [1] TRUE

# tweaked to make means differ to illustrate the test
v1pre1  <- c(2,1,1,0,0,0,0.0,0,1,0) # group1
v1pre2  <- c(3,1,0,0,0,1,0.0,1,0,0) # group2
mean(v1pre1) == mean(v1pre2)
#> [1] FALSE

t.test(v1pre1,v1pre2)
#> 
#>  Welch Two Sample t-test
#> 
#> data:  v1pre1 and v1pre2
#> t = -0.26414, df = 16.493, p-value = 0.7949
#> alternative hypothesis: true difference in means is not equal to 0
#> 95 percent confidence interval:
#>  -0.9006395  0.7006395
#> sample estimates:
#> mean of x mean of y 
#>       0.5       0.6
```

This tells us that we cannot reject $H_0$, that the true difference of  the means is zero.

Phil Spector, from Cal, has a [great explainer for the t-test](https://statistics.berkeley.edu/computing/r-t-tests)

What if you didn't want the mean but the score-weighted mean?