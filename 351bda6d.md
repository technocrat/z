---
date: 2020-12-29T23:27
---

# ZeroR classifier

[OneR](https://cran.r-project.org/web/packages/OneR/vignettes/OneR.html)

Category-oriented; discretizes continuous data. Omits NAs. Relies on [single attribute](https://link.springer.com/article/10.1023/A:1022631118932)

> 1.  Systems  designed  using the  "simplicity  first"  methodology  are guaranteed to produce rules  that  are  near-optimal  with  respect  to  simplicity.  If  the  accuracy  of  the  rule  is unsatisfactory,  then  there  does  not exist a satisfactory  simple  rule,  so to improve accuracy  one  must increase  the  complexity  of  the  rules  being  considered. 

	dat <- optbin(mtcars)
		model <- OneR(dat, verbose = TRUE)
		summary(model)
		plot(model)
		prediction <- predict(model, dat)
		eval_model(prediction, dat)
        

> In one word: we need a good baseline which builds “the best simple model” that strikes a balance between the best accuracy possible with a model that is still simple enough to understand: I have developed the OneR package for finding this sweet spot and thereby establishing a new baseline for classification models in Machine Learning (ML).

    library(OneR)
    library(randomForest)

    # https://blog.ephorie.de/oner-machine-learning-in-under-one-minute
    # https://blog.ephorie.de/zeror-the-simplest-possible-classifier-or-why-high-accuracy-can-be-misleading(OneR)

    ZeroR <- function(x, ...) {
      output <- OneR(cbind(dummy = TRUE, x[ncol(x)]), ...)
      class(output) <- c("ZeroR", "OneR")
      output
    }
    predict.ZeroR <- function(object, newdata, ...) {
      class(object) <- "OneR"
      predict(object, cbind(dummy = TRUE, newdata[ncol(newdata)]), ...)
    }

    dat <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data")

    dat <- data.frame(dat[ , 1:20], creditrisk = factor(dat[ , 21]))
    table(dat$creditrisk)


    set.seed(805)
    random <- sample(1:nrow(dat), 0.6 * nrow(dat))
    data_train <- dat[random, ]
    data_test <- dat[-random, ]

    #   The ZeroR classifier now takes the majority class (good credit risk) and uses it as the prediction every time! You have read correctly, it just predicts that every customer is a good credit risk!

    model <- ZeroR(data_train)
    summary(model)


    plot(model)

    prediction <- predict(model, data_test)
    eval_model(prediction, data_test)

    model <- OneR(optbin(data_train))
    summary(model)

    plot(model)

    prediction <- predict(model, data_test)
    eval_model(prediction, data_test)
    set.seed(78)

    model <- randomForest(creditrisk ~., data = data_train, ntree = 2000)
    prediction <- predicnvim colort(model, data_test)
    eval_model(prediction, data_test)


    dat <- optbin(iris)
    model <- OneR(dat, verbose = TRUE)
    summary(model)
    plot(model)
    prediction <- predict(model, dat)
    eval_model(prediction, dat)

[[R]]
[[[script]]]
[[ML]]